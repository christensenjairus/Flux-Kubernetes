apiVersion: helm.toolkit.fluxcd.io/v2beta2
kind: HelmRelease
metadata:
  name: cilium
  namespace: kube-system
spec:
  interval: 30m
  install:
    timeout: 20m # takes a while to install because every pod needs to roll out one at a time
  upgrade:
    timeout: 20m
  chart:
    spec:
      chart: cilium
      version: "1.15.x"
      sourceRef:
        kind: HelmRepository
        name: cilium
        namespace: kube-system
      interval: 12h
  values:
    cluster:
      id: 6
      name: "zeta"
    hubble:
      enabled: true
      relay:
        enabled: true
        nodeSelector:
          nodeclass: general
        prometheus:
          serviceMonitor:
            enabled: true
      ui:
        enabled: true
        nodeSelector:
          nodeclass: general
      metrics:
        serviceMonitor:
          enabled: true
        enabled:
          - dns
          - drop
          - flow
          - flows-to-world
          - httpV2
          - icmp
          - kafka
          - port-distribution
          - tcp
        dashboards:
          enabled: false
          # enabled: false # imported directly into victoriametrics
        enableOpenMetrics: true
    envoy:
      enabled: true
#      connectTimeoutSeconds: 10 # default is 2
#      idleTimeoutDurationSeconds: 180 # default is 60
      prometheus:
        enabled: true
        serviceMonitor:
          enabled: true
    prometheus:
      serviceMonitor:
        enabled: true
    operator:
      replicas: 1
      nodeSelector:
        nodeclass: general
      prometheus:
        serviceMonitor:
          enabled: true
      dashboards:
        enabled: false # imported directly into victoriametrics
    proxy:
      prometheus:
        enabled: true
    dashboards:
      enabled: false # imported directly into victoriametrics
    loadBalancer:
      l7:
        backend: envoy
    rolloutCiliumPods: true
    encryption:
      nodeEncryption: false # enable when necessary. I don't need it because I'm using a private network.
    ipv4:
      enabled: true
    ipv6:
      enabled: false
    ipam:
      mode: kubernetes
    gatewayAPI:
      enabled: true
    externalWorkloads:
      enabled: true
    clustermesh:
      useAPIServer: true
      apiserver:
        replicas: 2
        nodeSelector:
          nodeclass: general
        kvstoremesh:
          enabled: true
        metrics:
          enabled: true
          kvstoremesh:
            enabled: true
          etcd:
            enabled: true
          serviceMonitor:
            enabled: true
        service:
          type: LoadBalancer
          annotations:
            io.cilium/lb-ipam-ips: "10.0.6.254"
          loadBalancerIP: 10.0.6.254
    externalIPs:
      enabled: true
    nodePort:
      enabled: true
    hostPort:
      enabled: true
    l2announcements:
      enabled: false #true
      # Raising the lease and qps values decreases the number of API calls needed to keep the lease
      # See this (now fixed) bug: https://github.com/cilium/cilium/issues/26586#issuecomment-1620641960
      leaseDuration: 150s # 15s is default
      leaseRenewDeadline: 50s # 5s is default
      leaseRetryPeriod: 20s # 2s is default
      # These ^ values ensure a maximum of 200s to failover after failure. (minimum of 100s)
      # "The theoretical shortest time between failure and failover is leaseDuration - leaseRenewDeadline and the longest leaseDuration + leaseRenewDeadline.
      # So with the default values, failover occurs between 10s and 20s.
      # Each service incurs a CPU and network overhead, so clusters with smaller amounts of services can more easily afford faster failover times.
      # Larger clusters might need to increase parameters if the overhead is too high." - Docs (https://docs.cilium.io/en/stable/network/l2-announcements/)
    k8sClientRateLimit:
      qps: 30 # 10 is default
      burst: 60 # 20 is default
    k8sServiceHost: "10.0.6.100"
    k8sServicePort: 6443
    kubeProxyReplacement: strict
